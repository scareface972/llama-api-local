#!/bin/bash

echo "üöÄ Installation propre de l'API Llama.cpp"
echo "========================================"

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonction d'affichage avec couleur
print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Fonction de v√©rification d'erreur
check_error() {
    if [ $? -ne 0 ]; then
        print_error "$1"
        exit 1
    fi
}

# V√©rification de l'utilisateur
if [ "$EUID" -eq 0 ]; then
    print_error "Ne pas ex√©cuter ce script en tant que root"
    print_status "Utilisez un utilisateur normal avec sudo"
    exit 1
fi

print_status "Utilisateur: $(whoami)"
print_status "R√©pertoire: $(pwd)"

# ============================================================================
# √âTAPE 1: MISE √Ä JOUR DU SYST√àME
# ============================================================================
print_status "√âTAPE 1: Mise √† jour du syst√®me..."
sudo apt update && sudo apt upgrade -y
check_error "√âchec de la mise √† jour du syst√®me"

# ============================================================================
# √âTAPE 2: INSTALLATION DES D√âPENDANCES SYST√àME
# ============================================================================
print_status "√âTAPE 2: Installation des d√©pendances syst√®me..."

# D√©pendances syst√®me essentielles
sudo apt install -y \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    python3 \
    python3-pip \
    python3-venv \
    libopenblas-dev \
    liblapack-dev \
    gfortran \
    libssl-dev \
    libffi-dev \
    pkg-config \
    htop

check_error "√âchec de l'installation des d√©pendances syst√®me"

# ============================================================================
# √âTAPE 3: CR√âATION DE L'ENVIRONNEMENT VIRTUEL
# ============================================================================
print_status "√âTAPE 3: Configuration de l'environnement Python..."

# Suppression de l'ancien environnement si il existe
if [ -d "venv" ]; then
    print_warning "Environnement virtuel existe d√©j√†, suppression..."
    rm -rf venv
fi

# Cr√©ation d'un nouvel environnement virtuel
python3 -m venv venv
check_error "√âchec de la cr√©ation de l'environnement virtuel"

# Activation de l'environnement virtuel
source venv/bin/activate
check_error "√âchec de l'activation de l'environnement virtuel"

# V√©rification que l'environnement est activ√©
if [ -z "$VIRTUAL_ENV" ]; then
    print_error "L'environnement virtuel n'est pas activ√©"
    exit 1
fi

print_status "Environnement virtuel activ√© : $VIRTUAL_ENV"

# ============================================================================
# √âTAPE 4: INSTALLATION DES D√âPENDANCES PYTHON
# ============================================================================
print_status "√âTAPE 4: Installation des d√©pendances Python..."

# Mise √† jour de pip
python -m pip install --upgrade pip
check_error "√âchec de la mise √† jour de pip"

# Installation des d√©pendances de base
python -m pip install --upgrade setuptools wheel
check_error "√âchec de l'installation de setuptools/wheel"

# Installation s√©quentielle des d√©pendances
print_status "Installation s√©quentielle des d√©pendances..."

# 1. Numpy
print_status "1. Installation de numpy..."
python -m pip install "numpy>=1.24.0"
check_error "√âchec de l'installation de numpy"

# 2. FastAPI et Uvicorn
print_status "2. Installation de FastAPI et Uvicorn..."
python -m pip install "fastapi>=0.104.1" "uvicorn[standard]>=0.24.0"
check_error "√âchec de l'installation de FastAPI/Uvicorn"

# 3. Pydantic
print_status "3. Installation de Pydantic..."
python -m pip install "pydantic>=2.5.0"
check_error "√âchec de l'installation de Pydantic"

# 4. Autres d√©pendances web
print_status "4. Installation des d√©pendances web..."
python -m pip install "python-multipart>=0.0.6" "jinja2>=3.1.2" "aiofiles>=23.2.1"
check_error "√âchec de l'installation des d√©pendances web"

# 5. Psutil
print_status "5. Installation de psutil..."
python -m pip install "psutil>=5.9.6"
check_error "√âchec de l'installation de psutil"

# 6. PyTorch (CPU uniquement pour simplifier)
print_status "6. Installation de PyTorch (CPU)..."
python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
check_error "√âchec de l'installation de PyTorch"

# 7. Transformers
print_status "7. Installation de Transformers..."
python -m pip install "transformers>=4.36.0" "sentencepiece>=0.1.99"
check_error "√âchec de l'installation de Transformers"

# 8. Llama-cpp-python (CPU uniquement)
print_status "8. Installation de llama-cpp-python..."
python -m pip install llama-cpp-python --no-cache-dir
check_error "√âchec de l'installation de llama-cpp-python"

# ============================================================================
# √âTAPE 5: INSTALLATION DE LLAMA.CPP
# ============================================================================
print_status "√âTAPE 5: Installation de llama.cpp..."

# Suppression de l'ancienne installation si elle existe
if [ -d "llama.cpp" ]; then
    print_warning "llama.cpp existe d√©j√†, suppression..."
    rm -rf llama.cpp
fi

# Clonage de llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
check_error "√âchec du clonage de llama.cpp"

cd llama.cpp

# Compilation avec CMake (nouvelle m√©thode)
print_status "Compilation avec CMake (nouvelle m√©thode)..."
mkdir -p build
cd build

# Configuration CMake avec optimisations
cmake .. -DLLAMA_BLAS=ON -DLLAMA_OPENBLAS=ON -DGGML_NATIVE=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_METAL=OFF -DLLAMA_CUBLAS=OFF
check_error "√âchec de la configuration CMake"

# Compilation
make -j$(nproc)
check_error "√âchec de la compilation de llama.cpp"

# Retour au r√©pertoire principal
cd ../..

# ============================================================================
# √âTAPE 6: CR√âATION DE LA STRUCTURE DU PROJET
# ============================================================================
print_status "√âTAPE 6: Cr√©ation de la structure du projet..."
mkdir -p models
mkdir -p logs
mkdir -p static
mkdir -p templates
mkdir -p config

# ============================================================================
# √âTAPE 7: CR√âATION DES FICHIERS DE CONFIGURATION
# ============================================================================
print_status "√âTAPE 7: Cr√©ation des fichiers de configuration..."

# Cr√©ation du fichier requirements.txt
cat > requirements.txt << 'EOF'
fastapi>=0.104.1
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
python-multipart>=0.0.6
jinja2>=3.1.2
aiofiles>=23.2.1
psutil>=5.9.6
torch>=2.0.0
transformers>=4.36.0
sentencepiece>=0.1.99
llama-cpp-python>=0.2.0
numpy>=1.24.0
EOF

# Cr√©ation du fichier de configuration
cat > config.py << 'EOF'
import os
from pathlib import Path

# Configuration de base
BASE_DIR = Path(__file__).parent
MODELS_DIR = BASE_DIR / "models"
LOGS_DIR = BASE_DIR / "logs"
STATIC_DIR = BASE_DIR / "static"
TEMPLATES_DIR = BASE_DIR / "templates"

# Configuration du serveur
HOST = "0.0.0.0"
PORT = 8000
WORKERS = 1
RELOAD = True

# Configuration du mod√®le
DEFAULT_MODEL = "llama-2-7b-chat.gguf"
MODEL_PATH = MODELS_DIR / DEFAULT_MODEL

# Configuration CUDA (d√©sactiv√©e par d√©faut)
USE_CUDA = False
CUDA_LAYERS = 0

# Configuration de la g√©n√©ration
MAX_TOKENS = 2048
TEMPERATURE = 0.7
TOP_P = 0.9
TOP_K = 40
REPEAT_PENALTY = 1.1

# Configuration des logs
LOG_LEVEL = "INFO"
LOG_FILE = LOGS_DIR / "llama_api.log"

# Cr√©ation des r√©pertoires si ils n'existent pas
for directory in [MODELS_DIR, LOGS_DIR, STATIC_DIR, TEMPLATES_DIR]:
    directory.mkdir(exist_ok=True)
EOF

# Cr√©ation du fichier principal de l'API
cat > llama_api.py << 'EOF'
#!/usr/bin/env python3
"""
API Llama.cpp - Serveur FastAPI pour l'inference de mod√®les de langage
"""

import os
import sys
import logging
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException, Request, BackgroundTasks
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field
import psutil

# Ajout du r√©pertoire parent au path pour les imports
sys.path.append(str(Path(__file__).parent))

try:
    from llama_cpp import Llama
    from config import *
except ImportError as e:
    print(f"Erreur d'import: {e}")
    print("V√©rifiez que toutes les d√©pendances sont install√©es")
    sys.exit(1)

# Configuration des logs
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Variables globales
llm: Optional[Llama] = None
model_loaded = False

class ChatRequest(BaseModel):
    message: str = Field(..., description="Message de l'utilisateur")
    system_prompt: Optional[str] = Field(None, description="Prompt syst√®me optionnel")
    max_tokens: Optional[int] = Field(MAX_TOKENS, description="Nombre maximum de tokens")
    temperature: Optional[float] = Field(TEMPERATURE, description="Temp√©rature pour la g√©n√©ration")
    top_p: Optional[float] = Field(TOP_P, description="Top-p sampling")
    top_k: Optional[int] = Field(TOP_K, description="Top-k sampling")
    repeat_penalty: Optional[float] = Field(REPEAT_PENALTY, description="P√©nalit√© de r√©p√©tition")

class ChatResponse(BaseModel):
    response: str
    tokens_used: int
    model_info: Dict[str, Any]

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    system_info: Dict[str, Any]

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestion du cycle de vie de l'application"""
    global llm, model_loaded
    
    logger.info("D√©marrage de l'API Llama.cpp...")
    
    # Chargement du mod√®le en arri√®re-plan
    asyncio.create_task(load_model_async())
    
    yield
    
    logger.info("Arr√™t de l'API Llama.cpp...")
    if llm:
        del llm

# Cr√©ation de l'application FastAPI
app = FastAPI(
    title="API Llama.cpp",
    description="API REST pour l'inference de mod√®les de langage avec llama.cpp",
    version="1.0.0",
    lifespan=lifespan
)

# Montage des fichiers statiques et templates
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")
templates = Jinja2Templates(directory=TEMPLATES_DIR)

async def load_model_async():
    """Chargement asynchrone du mod√®le"""
    global llm, model_loaded
    
    try:
        logger.info("Chargement du mod√®le en cours...")
        
        if not MODEL_PATH.exists():
            logger.error(f"Mod√®le non trouv√©: {MODEL_PATH}")
            return
        
        # Configuration du mod√®le
        model_config = {
            "model_path": str(MODEL_PATH),
            "n_ctx": 4096,
            "n_batch": 512,
            "n_threads": os.cpu_count(),
            "verbose": False
        }
        
        # Ajout de la configuration CUDA si disponible
        if USE_CUDA:
            model_config.update({
                "n_gpu_layers": CUDA_LAYERS,
                "main_gpu": 0
            })
            logger.info("Configuration CUDA activ√©e")
        
        llm = Llama(**model_config)
        model_loaded = True
        logger.info("‚úÖ Mod√®le charg√© avec succ√®s")
        
    except Exception as e:
        logger.error(f"Erreur lors du chargement du mod√®le: {e}")
        model_loaded = False

@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """Page d'accueil"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """V√©rification de l'√©tat du serveur"""
    system_info = {
        "cpu_count": psutil.cpu_count(),
        "memory_total": psutil.virtual_memory().total,
        "memory_available": psutil.virtual_memory().available,
        "disk_usage": psutil.disk_usage('/').percent
    }
    
    return HealthResponse(
        status="healthy" if model_loaded else "loading",
        model_loaded=model_loaded,
        system_info=system_info
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Endpoint de chat avec le mod√®le"""
    global llm, model_loaded
    
    if not model_loaded or llm is None:
        raise HTTPException(status_code=503, detail="Mod√®le non encore charg√©")
    
    try:
        # Pr√©paration du prompt
        if request.system_prompt:
            full_prompt = f"[INST] <<SYS>>\n{request.system_prompt}\n<</SYS>>\n\n{request.message} [/INST]"
        else:
            full_prompt = f"[INST] {request.message} [/INST]"
        
        # G√©n√©ration de la r√©ponse
        response = llm(
            full_prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            top_k=request.top_k,
            repeat_penalty=request.repeat_penalty,
            stop=["[INST]", "</s>"]
        )
        
        # Extraction de la r√©ponse
        generated_text = response['choices'][0]['text'].strip()
        tokens_used = response['usage']['total_tokens']
        
        # Informations sur le mod√®le
        model_info = {
            "model_name": MODEL_PATH.name,
            "context_length": response['usage']['prompt_tokens'] + response['usage']['completion_tokens'],
            "generation_time": response.get('generation_time', 0)
        }
        
        return ChatResponse(
            response=generated_text,
            tokens_used=tokens_used,
            model_info=model_info
        )
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur de g√©n√©ration: {str(e)}")

@app.get("/models")
async def list_models():
    """Liste des mod√®les disponibles"""
    models = []
    for model_file in MODELS_DIR.glob("*.gguf"):
        models.append({
            "name": model_file.name,
            "size": model_file.stat().st_size,
            "path": str(model_file)
        })
    return {"models": models}

@app.get("/system/info")
async def system_info():
    """Informations syst√®me"""
    return {
        "cpu_count": psutil.cpu_count(),
        "memory": {
            "total": psutil.virtual_memory().total,
            "available": psutil.virtual_memory().available,
            "percent": psutil.virtual_memory().percent
        },
        "disk": {
            "total": psutil.disk_usage('/').total,
            "free": psutil.disk_usage('/').free,
            "percent": psutil.disk_usage('/').percent
        },
        "model_loaded": model_loaded
    }

if __name__ == "__main__":
    logger.info(f"D√©marrage du serveur sur {HOST}:{PORT}")
    uvicorn.run(
        "llama_api:app",
        host=HOST,
        port=PORT,
        reload=RELOAD,
        workers=WORKERS
    )
EOF

# Cr√©ation du template HTML
mkdir -p templates
cat > templates/index.html << 'EOF'
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Llama.cpp</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }
        .header h1 {
            margin: 0;
            font-size: 2.5em;
            font-weight: 300;
        }
        .header p {
            margin: 10px 0 0 0;
            opacity: 0.9;
            font-size: 1.1em;
        }
        .content {
            padding: 30px;
        }
        .chat-container {
            display: flex;
            flex-direction: column;
            height: 500px;
            border: 2px solid #e1e5e9;
            border-radius: 10px;
            overflow: hidden;
        }
        .chat-messages {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
            background: #f8f9fa;
        }
        .message {
            margin-bottom: 15px;
            padding: 15px;
            border-radius: 10px;
            max-width: 80%;
        }
        .message.user {
            background: #007bff;
            color: white;
            margin-left: auto;
        }
        .message.assistant {
            background: white;
            border: 1px solid #e1e5e9;
        }
        .chat-input {
            display: flex;
            padding: 20px;
            background: white;
            border-top: 1px solid #e1e5e9;
        }
        .chat-input input {
            flex: 1;
            padding: 15px;
            border: 2px solid #e1e5e9;
            border-radius: 25px;
            font-size: 16px;
            outline: none;
        }
        .chat-input input:focus {
            border-color: #667eea;
        }
        .chat-input button {
            margin-left: 10px;
            padding: 15px 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 16px;
            transition: transform 0.2s;
        }
        .chat-input button:hover {
            transform: translateY(-2px);
        }
        .chat-input button:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }
        .status {
            text-align: center;
            margin-bottom: 20px;
            padding: 10px;
            border-radius: 5px;
            font-weight: bold;
        }
        .status.loading {
            background: #fff3cd;
            color: #856404;
            border: 1px solid #ffeaa7;
        }
        .status.ready {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        .status.error {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        .info-panel {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-top: 20px;
        }
        .info-panel h3 {
            margin-top: 0;
            color: #495057;
        }
        .info-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }
        .info-item {
            background: white;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }
        .info-label {
            font-weight: bold;
            color: #495057;
            font-size: 0.9em;
        }
        .info-value {
            font-size: 1.1em;
            color: #212529;
            margin-top: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ü§ñ API Llama.cpp</h1>
            <p>Interface de chat avec mod√®le de langage local</p>
        </div>
        
        <div class="content">
            <div id="status" class="status loading">
                üîÑ Chargement du mod√®le en cours...
            </div>
            
            <div class="chat-container">
                <div id="messages" class="chat-messages">
                    <div class="message assistant">
                        üëã Bonjour ! Je suis votre assistant IA local. Posez-moi vos questions !
                    </div>
                </div>
                <div class="chat-input">
                    <input type="text" id="messageInput" placeholder="Tapez votre message..." disabled>
                    <button id="sendButton" disabled>Envoyer</button>
                </div>
            </div>
            
            <div class="info-panel">
                <h3>üìä Informations syst√®me</h3>
                <div id="systemInfo" class="info-grid">
                    <div class="info-item">
                        <div class="info-label">√âtat du mod√®le</div>
                        <div class="info-value">Chargement...</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">CPU</div>
                        <div class="info-value">-</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">M√©moire</div>
                        <div class="info-value">-</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">Disque</div>
                        <div class="info-value">-</div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        let isModelReady = false;
        
        // V√©rification de l'√©tat du serveur
        async function checkHealth() {
            try {
                const response = await fetch('/health');
                const data = await response.json();
                
                if (data.model_loaded) {
                    document.getElementById('status').className = 'status ready';
                    document.getElementById('status').textContent = '‚úÖ Mod√®le pr√™t !';
                    document.getElementById('messageInput').disabled = false;
                    document.getElementById('sendButton').disabled = false;
                    isModelReady = true;
                } else {
                    document.getElementById('status').className = 'status loading';
                    document.getElementById('status').textContent = 'üîÑ Chargement du mod√®le en cours...';
                }
                
                // Mise √† jour des informations syst√®me
                updateSystemInfo(data.system_info);
                
            } catch (error) {
                document.getElementById('status').className = 'status error';
                document.getElementById('status').textContent = '‚ùå Erreur de connexion au serveur';
            }
        }
        
        // Mise √† jour des informations syst√®me
        function updateSystemInfo(info) {
            const cpuElement = document.querySelector('.info-item:nth-child(2) .info-value');
            const memoryElement = document.querySelector('.info-item:nth-child(3) .info-value');
            const diskElement = document.querySelector('.info-item:nth-child(4) .info-value');
            
            cpuElement.textContent = `${info.cpu_count} c≈ìurs`;
            memoryElement.textContent = `${Math.round(info.memory_available / 1024 / 1024 / 1024)} GB libres`;
            diskElement.textContent = `${Math.round(info.disk_usage)}% utilis√©`;
        }
        
        // Envoi de message
        async function sendMessage() {
            if (!isModelReady) return;
            
            const input = document.getElementById('messageInput');
            const message = input.value.trim();
            
            if (!message) return;
            
            // Ajout du message utilisateur
            addMessage(message, 'user');
            input.value = '';
            
            // D√©sactivation de l'interface
            document.getElementById('sendButton').disabled = true;
            document.getElementById('messageInput').disabled = true;
            
            try {
                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: message,
                        max_tokens: 2048,
                        temperature: 0.7
                    })
                });
                
                const data = await response.json();
                
                if (response.ok) {
                    addMessage(data.response, 'assistant');
                } else {
                    addMessage(`‚ùå Erreur: ${data.detail}`, 'assistant');
                }
                
            } catch (error) {
                addMessage(`‚ùå Erreur de connexion: ${error.message}`, 'assistant');
            } finally {
                // R√©activation de l'interface
                document.getElementById('sendButton').disabled = false;
                document.getElementById('messageInput').disabled = false;
                document.getElementById('messageInput').focus();
            }
        }
        
        // Ajout d'un message dans le chat
        function addMessage(text, type) {
            const messagesDiv = document.getElementById('messages');
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${type}`;
            messageDiv.textContent = text;
            messagesDiv.appendChild(messageDiv);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }
        
        // √âv√©nements
        document.getElementById('sendButton').addEventListener('click', sendMessage);
        document.getElementById('messageInput').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });
        
        // V√©rification p√©riodique de l'√©tat
        checkHealth();
        setInterval(checkHealth, 5000);
        
        // R√©cup√©ration des informations syst√®me
        async function updateSystemInfo() {
            try {
                const response = await fetch('/system/info');
                const data = await response.json();
                updateSystemInfo(data);
            } catch (error) {
                console.error('Erreur lors de la r√©cup√©ration des informations syst√®me:', error);
            }
        }
        
        // Mise √† jour des informations syst√®me toutes les 10 secondes
        setInterval(updateSystemInfo, 10000);
    </script>
</body>
</html>
EOF

# ============================================================================
# √âTAPE 8: CONFIGURATION DES PERMISSIONS
# ============================================================================
print_status "√âTAPE 8: Configuration des permissions..."
chmod +x *.sh
chmod +x llama_api.py

# ============================================================================
# √âTAPE 9: CONFIGURATION DU SERVICE SYSTEMD
# ============================================================================
print_status "√âTAPE 9: Configuration du service systemd..."

# D√©tection de l'utilisateur et du chemin
CURRENT_USER=$(whoami)
CURRENT_GROUP=$(id -gn)
PROJECT_PATH=$(pwd)

# Cr√©ation du fichier service
cat > llama-api.service << EOF
[Unit]
Description=Llama.cpp API Server
After=network.target

[Service]
Type=simple
User=$CURRENT_USER
Group=$CURRENT_GROUP
WorkingDirectory=$PROJECT_PATH
Environment=PATH=$PROJECT_PATH/venv/bin
ExecStart=$PROJECT_PATH/venv/bin/python $PROJECT_PATH/llama_api.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# Copie du fichier service vers systemd
sudo cp llama-api.service /etc/systemd/system/
check_error "√âchec de la copie du fichier service"

# Rechargement de systemd
sudo systemctl daemon-reload
check_error "√âchec du rechargement de systemd"

# Activation du service
sudo systemctl enable llama-api.service
check_error "√âchec de l'activation du service"

print_status "‚úÖ Service systemd configur√© et activ√©"

# ============================================================================
# √âTAPE 10: CONFIGURATION DU FIREWALL
# ============================================================================
print_status "√âTAPE 10: Configuration du firewall..."
if command -v ufw &> /dev/null; then
    sudo ufw allow 8000/tcp
    print_status "‚úÖ Port 8000 ouvert dans le firewall"
else
    print_warning "UFW non install√©, configurez manuellement le port 8000"
fi

# ============================================================================
# √âTAPE 11: V√âRIFICATION FINALE
# ============================================================================
print_status "√âTAPE 11: V√©rification finale..."

# V√©rification de l'environnement virtuel
if [ -d "venv" ] && [ -f "venv/bin/activate" ]; then
    print_status "‚úÖ Environnement virtuel cr√©√©"
else
    print_error "‚ùå Environnement virtuel manquant"
    exit 1
fi

# V√©rification de llama.cpp
if [ -d "llama.cpp" ] && [ -f "llama.cpp/build/main" ]; then
    print_status "‚úÖ llama.cpp compil√©"
else
    print_error "‚ùå llama.cpp manquant ou non compil√©"
    exit 1
fi

# V√©rification des d√©pendances Python
source venv/bin/activate
python -c "
import sys
packages = ['fastapi', 'uvicorn', 'pydantic', 'numpy', 'torch', 'transformers', 'llama_cpp']
missing = []
for pkg in packages:
    try:
        __import__(pkg)
        print(f'‚úÖ {pkg} install√©')
    except ImportError:
        missing.append(pkg)
        print(f'‚ùå {pkg} manquant')

if missing:
    print(f'\\n‚ùå Packages manquants: {missing}')
    sys.exit(1)
else:
    print('\\n‚úÖ Toutes les d√©pendances sont install√©es')
"

if [ $? -eq 0 ]; then
    print_status "‚úÖ Installation termin√©e avec succ√®s !"
else
    print_error "‚ùå Certaines d√©pendances sont manquantes"
    exit 1
fi

# ============================================================================
# FINALISATION
# ============================================================================
echo ""
echo "üéâ INSTALLATION PROPRE TERMIN√âE AVEC SUCC√àS !"
echo "============================================="
echo ""
echo "üìã Prochaines √©tapes :"
echo "1. T√©l√©charger le mod√®le : ./download_model.sh"
echo "2. Tester le serveur : ./start_server.sh"
echo "3. Ou d√©marrer le service : sudo systemctl start llama-api"
echo "4. V√©rifier le statut : sudo systemctl status llama-api"
echo "5. Voir les logs : sudo journalctl -u llama-api -f"
echo ""
echo "üåê URLs d'acc√®s :"
echo "   ‚Ä¢ Interface Web : http://localhost:8000"
echo "   ‚Ä¢ Documentation API : http://localhost:8000/docs"
echo "   ‚Ä¢ Health Check : http://localhost:8000/health"
echo ""
echo "üîß Commandes utiles :"
echo "   ‚Ä¢ D√©marrer : sudo systemctl start llama-api"
echo "   ‚Ä¢ Arr√™ter : sudo systemctl stop llama-api"
echo "   ‚Ä¢ Red√©marrer : sudo systemctl restart llama-api"
echo "   ‚Ä¢ Statut : sudo systemctl status llama-api"
echo "   ‚Ä¢ Logs : sudo journalctl -u llama-api -f"
echo ""
echo "üéØ Votre API Llama.cpp est pr√™te !" 