#!/bin/bash

echo "ğŸš€ DÃ©marrage de l'API Llama.cpp optimisÃ©e"
echo "========================================="

# VÃ©rification de l'environnement virtuel
if [ ! -d "venv" ]; then
    echo "âŒ Environnement virtuel non trouvÃ©"
    echo "ğŸ”§ ExÃ©cutez d'abord : ./install.sh"
    exit 1
fi

# Activation de l'environnement virtuel
echo "ğŸ Activation de l'environnement virtuel..."
source venv/bin/activate

# VÃ©rification des dÃ©pendances
echo "ğŸ“¦ VÃ©rification des dÃ©pendances..."
if ! python -c "import fastapi, uvicorn, llama_cpp" 2>/dev/null; then
    echo "âŒ DÃ©pendances manquantes"
    echo "ğŸ”§ ExÃ©cutez : pip install -r requirements.txt"
    exit 1
fi

# VÃ©rification du modÃ¨le
echo "ğŸ¤– VÃ©rification du modÃ¨le..."
if [ ! -f "models/llama-2-7b-chat.gguf" ]; then
    echo "âŒ ModÃ¨le non trouvÃ©"
    echo "ğŸ“¥ ExÃ©cutez d'abord : ./download_model.sh"
    exit 1
fi

# VÃ©rification de llama.cpp
if [ ! -d "llama.cpp" ]; then
    echo "âŒ llama.cpp non trouvÃ©"
    echo "ğŸ”§ ExÃ©cutez d'abord : ./install.sh"
    exit 1
fi

# VÃ©rification de la compilation
if [ ! -f "llama.cpp/main" ]; then
    echo "âš ï¸  llama.cpp non compilÃ©, compilation en cours..."
    cd llama.cpp
    make clean
    make LLAMA_CUBLAS=1 LLAMA_AVX=1 LLAMA_AVX2=1 LLAMA_F16C=1 LLAMA_FMA=1 LLAMA_BLAS=1 LLAMA_OPENBLAS=1 -j$(nproc)
    cd ..
fi

# VÃ©rification de CUDA
echo "ğŸ® VÃ©rification de CUDA..."
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… NVIDIA GPU dÃ©tectÃ©"
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
else
    echo "âš ï¸  NVIDIA GPU non dÃ©tectÃ©, utilisation CPU uniquement"
fi

# VÃ©rification de la mÃ©moire
echo "ğŸ’¾ VÃ©rification de la mÃ©moire..."
TOTAL_RAM=$(free -g | awk '/^Mem:/{print $2}')
AVAILABLE_RAM=$(free -g | awk '/^Mem:/{print $7}')
echo "ğŸ“Š RAM totale: ${TOTAL_RAM}GB, Disponible: ${AVAILABLE_RAM}GB"

if [ $AVAILABLE_RAM -lt 4 ]; then
    echo "âš ï¸  Attention: Moins de 4GB de RAM disponible"
    echo "ğŸ’¡ Fermez d'autres applications pour de meilleures performances"
fi

# CrÃ©ation des rÃ©pertoires nÃ©cessaires
echo "ğŸ“ CrÃ©ation des rÃ©pertoires..."
mkdir -p logs
mkdir -p static
mkdir -p templates

# VÃ©rification des permissions
echo "ğŸ” VÃ©rification des permissions..."
chmod +x llama_api.py
chmod +x config.py

# Affichage de la configuration
echo ""
echo "âš™ï¸  Configuration dÃ©tectÃ©e :"
echo "   â€¢ ModÃ¨le: llama-2-7b-chat.gguf"
echo "   â€¢ API: FastAPI + Uvicorn"
echo "   â€¢ Interface: Web moderne avec coloration syntaxique"
echo "   â€¢ Optimisations: i5 + GTX 950M + 8GB RAM"
echo ""

# DÃ©marrage du serveur
echo "ğŸŒ DÃ©marrage du serveur..."
echo "ğŸ“¡ L'API sera accessible sur : http://localhost:8000"
echo "ğŸ“– Documentation API : http://localhost:8000/docs"
echo "ğŸ”§ Interface web : http://localhost:8000"
echo ""
echo "ğŸ›‘ Pour arrÃªter le serveur, appuyez sur Ctrl+C"
echo ""

# DÃ©marrage avec uvicorn
python llama_api.py 