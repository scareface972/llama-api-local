#!/bin/bash

echo "üöÄ D√©marrage de l'API Llama.cpp optimis√©e"
echo "========================================="

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonction d'affichage avec couleur
print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Fonction de v√©rification d'erreur
check_error() {
    if [ $? -ne 0 ]; then
        print_error "$1"
        exit 1
    fi
}

# V√©rification de l'environnement virtuel
print_status "V√©rification de l'environnement virtuel..."
if [ ! -d "venv" ]; then
    print_error "Environnement virtuel non trouv√©"
    print_error "Ex√©cutez d'abord : ./install.sh"
    exit 1
fi

if [ ! -f "venv/bin/activate" ]; then
    print_error "Script d'activation de l'environnement virtuel non trouv√©"
    print_error "Ex√©cutez d'abord : ./install.sh"
    exit 1
fi

# Activation de l'environnement virtuel
print_status "Activation de l'environnement virtuel..."
source venv/bin/activate
check_error "√âchec de l'activation de l'environnement virtuel"

# V√©rification que l'environnement est activ√©
if [ -z "$VIRTUAL_ENV" ]; then
    print_error "L'environnement virtuel n'est pas activ√©"
    exit 1
fi

print_status "Environnement virtuel activ√© : $VIRTUAL_ENV"

# V√©rification des d√©pendances
print_status "V√©rification des d√©pendances..."
if ! python -c "import fastapi, uvicorn, llama_cpp" 2>/dev/null; then
    print_error "D√©pendances manquantes"
    print_error "Ex√©cutez : pip install -r requirements.txt"
    print_error "Ou relancez : ./install.sh"
    exit 1
fi

print_status "‚úÖ D√©pendances v√©rifi√©es"

# V√©rification du mod√®le
print_status "V√©rification du mod√®le..."
if [ ! -f "models/llama-2-7b-chat.gguf" ]; then
    print_warning "Mod√®le non trouv√©"
    print_status "Ex√©cutez d'abord : ./download_model.sh"
    print_status "Ou t√©l√©chargez manuellement le mod√®le dans le dossier models/"
    read -p "Voulez-vous continuer sans mod√®le ? (y/N): " -n 1 -r
    echo ""
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        print_error "D√©marrage annul√©"
        exit 1
    fi
else
    print_status "‚úÖ Mod√®le trouv√©"
fi

# V√©rification de llama.cpp
if [ ! -d "llama.cpp" ]; then
    print_error "llama.cpp non trouv√©"
    print_error "Ex√©cutez d'abord : ./install.sh"
    exit 1
fi

# V√©rification de la compilation
if [ ! -f "llama.cpp/main" ]; then
    print_warning "llama.cpp non compil√©, compilation en cours..."
    cd llama.cpp
    make clean
    make LLAMA_CUBLAS=1 LLAMA_AVX=1 LLAMA_AVX2=1 LLAMA_F16C=1 LLAMA_FMA=1 LLAMA_BLAS=1 LLAMA_OPENBLAS=1 -j$(nproc)
    check_error "√âchec de la compilation de llama.cpp"
    cd ..
    print_status "‚úÖ llama.cpp compil√©"
else
    print_status "‚úÖ llama.cpp d√©j√† compil√©"
fi

# V√©rification de CUDA
print_status "V√©rification de CUDA..."
if command -v nvidia-smi &> /dev/null; then
    print_status "‚úÖ NVIDIA GPU d√©tect√©"
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits
else
    print_warning "‚ö†Ô∏è  NVIDIA GPU non d√©tect√©, utilisation CPU uniquement"
fi

# V√©rification de la m√©moire
print_status "V√©rification de la m√©moire..."
TOTAL_RAM=$(free -g | awk '/^Mem:/{print $2}')
AVAILABLE_RAM=$(free -g | awk '/^Mem:/{print $7}')
echo "üìä RAM totale: ${TOTAL_RAM}GB, Disponible: ${AVAILABLE_RAM}GB"

if [ $AVAILABLE_RAM -lt 4 ]; then
    print_warning "Attention: Moins de 4GB de RAM disponible"
    print_warning "Fermez d'autres applications pour de meilleures performances"
fi

# Cr√©ation des r√©pertoires n√©cessaires
print_status "Cr√©ation des r√©pertoires..."
mkdir -p logs
mkdir -p static
mkdir -p templates

# V√©rification des permissions
print_status "V√©rification des permissions..."
chmod +x llama_api.py
chmod +x config.py

# Affichage de la configuration
echo ""
print_status "Configuration d√©tect√©e :"
echo "   ‚Ä¢ Mod√®le: llama-2-7b-chat.gguf"
echo "   ‚Ä¢ API: FastAPI + Uvicorn"
echo "   ‚Ä¢ Interface: Web moderne avec coloration syntaxique"
echo "   ‚Ä¢ Optimisations: i5 + GTX 950M + 8GB RAM"
echo "   ‚Ä¢ Acc√®s r√©seau: 0.0.0.0:8000"
echo ""

# D√©marrage du serveur
print_status "D√©marrage du serveur..."
echo "üì° L'API sera accessible sur :"
echo "   ‚Ä¢ Local: http://localhost:8000"
echo "   ‚Ä¢ R√©seau: http://[VOTRE_IP]:8000"
echo "üìñ Documentation API : http://localhost:8000/docs"
echo "üîß Interface web : http://localhost:8000"
echo "üè• Health Check : http://localhost:8000/health"
echo ""
echo "üõë Pour arr√™ter le serveur, appuyez sur Ctrl+C"
echo ""

# D√©marrage avec uvicorn
python llama_api.py 