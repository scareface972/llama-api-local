# ü§ñ API Llama.cpp - Installation Propre

Une API REST compl√®te pour l'inference de mod√®les de langage avec llama.cpp, optimis√©e pour Ubuntu et facile √† installer.

## üéØ Caract√©ristiques

- **API REST** avec FastAPI
- **Interface web moderne** avec chat en temps r√©el
- **Support multi-mod√®les** (Llama-2, CodeLlama, Mistral, etc.)
- **Optimisations CPU** pour performances maximales
- **Service systemd** pour d√©marrage automatique
- **Interface r√©seau** accessible depuis d'autres appareils
- **Monitoring syst√®me** int√©gr√©

## üöÄ Installation Rapide

### 1. D√©sinstallation (si n√©cessaire)
```bash
# Si vous avez une installation pr√©c√©dente
./uninstall.sh
```

### 2. Installation propre
```bash
# Installation compl√®te
./install_clean.sh
```

### 3. T√©l√©chargement du mod√®le
```bash
# T√©l√©chargement interactif d'un mod√®le
./download_model.sh
```

### 4. D√©marrage du serveur
```bash
# D√©marrage manuel
./start_server.sh

# Ou d√©marrage en service
sudo systemctl start llama-api
```

## üìã Pr√©requis

- **OS** : Ubuntu 20.04+ (test√© sur Ubuntu 24.04)
- **RAM** : 8GB minimum (16GB recommand√©)
- **Espace disque** : 10GB minimum
- **CPU** : x86_64 avec support AVX2
- **R√©seau** : Connexion internet pour t√©l√©charger les mod√®les

## üîß Scripts Disponibles

| Script | Description |
|--------|-------------|
| `install_clean.sh` | Installation compl√®te et propre |
| `uninstall.sh` | D√©sinstallation compl√®te |
| `download_model.sh` | T√©l√©chargement interactif de mod√®les |
| `start_server.sh` | D√©marrage du serveur |
| `network-info.sh` | Informations r√©seau |

## üåê Acc√®s

Une fois le serveur d√©marr√© :

- **Interface Web** : http://localhost:8000
- **Documentation API** : http://localhost:8000/docs
- **Health Check** : http://localhost:8000/health
- **Acc√®s r√©seau** : http://[VOTRE_IP]:8000

## üì¶ Mod√®les Support√©s

Le script `download_model.sh` propose plusieurs mod√®les :

1. **Llama-2-7B-Chat** (4.37 GB) - Recommand√© pour d√©buter
2. **Llama-2-13B-Chat** (7.87 GB) - Plus performant, plus lent
3. **Llama-2-7B** (4.37 GB) - Mod√®le de base
4. **CodeLlama-7B-Instruct** (4.37 GB) - Sp√©cialis√© code
5. **Mistral-7B-Instruct** (4.37 GB) - Tr√®s performant
6. **T√©l√©chargement personnalis√©** - URL personnalis√©e

## üõ†Ô∏è Gestion du Service

```bash
# D√©marrer le service
sudo systemctl start llama-api

# Arr√™ter le service
sudo systemctl stop llama-api

# Red√©marrer le service
sudo systemctl restart llama-api

# V√©rifier le statut
sudo systemctl status llama-api

# Voir les logs
sudo journalctl -u llama-api -f

# Activer le d√©marrage automatique
sudo systemctl enable llama-api
```

## üîç D√©pannage

### Probl√®mes courants

1. **Port 8000 d√©j√† utilis√©**
   ```bash
   sudo lsof -i :8000
   sudo kill -9 [PID]
   ```

2. **Mod√®le non trouv√©**
   ```bash
   ./download_model.sh
   ```

3. **D√©pendances manquantes**
   ```bash
   ./install_clean.sh
   ```

4. **Permissions insuffisantes**
   ```bash
   chmod +x *.sh
   ```

### Logs et diagnostic

```bash
# Logs de l'application
tail -f logs/llama_api.log

# Logs syst√®me
sudo journalctl -u llama-api -f

# Informations syst√®me
./network-info.sh
```

## üîí S√©curit√©

- L'API est accessible sur le r√©seau local (0.0.0.0:8000)
- Configurez votre firewall si n√©cessaire
- Pour un acc√®s externe, utilisez un reverse proxy avec authentification

## üìä Performance

### Configuration recommand√©e
- **CPU** : 4+ c≈ìurs
- **RAM** : 16GB
- **Espace disque** : SSD recommand√©
- **R√©seau** : 100Mbps minimum

### Optimisations automatiques
- Compilation optimis√©e pour votre CPU
- Gestion intelligente de la m√©moire
- Quantisation Q4_K_M pour √©quilibre performance/taille

## ü§ù Contribution

Ce projet est maintenu par la communaut√©. Pour contribuer :

1. Fork le projet
2. Cr√©ez une branche pour votre fonctionnalit√©
3. Committez vos changements
4. Poussez vers la branche
5. Ouvrez une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de d√©tails.

## üôè Remerciements

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Moteur d'inference
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web
- [TheBloke](https://huggingface.co/TheBloke) - Mod√®les quantifi√©s
- [Hugging Face](https://huggingface.co/) - H√©bergement des mod√®les

---

**üéØ Votre API Llama.cpp est pr√™te !** 

Pour commencer, ex√©cutez simplement :
```bash
./install_clean.sh
```
