# ğŸ¤– API Llama.cpp Locale - OptimisÃ©e pour i5 + GTX 950M

Une API locale complÃ¨te utilisant llama.cpp, optimisÃ©e spÃ©cifiquement pour votre configuration matÃ©rielle (Intel i5 + NVIDIA GTX 950M + 8GB RAM). Cette solution offre une interface web moderne avec coloration syntaxique pour le code et une utilisation optimisÃ©e de la RAM et VRAM.

## ğŸ¯ CaractÃ©ristiques

### ğŸš€ Performance OptimisÃ©e
- **CPU i5** : Utilisation intelligente des cÅ“urs disponibles
- **GPU GTX 950M** : AccÃ©lÃ©ration CUDA avec 20 couches GPU
- **RAM 8GB + VRAM 4GB** : Gestion hybride optimisÃ©e
- **Quantisation Q4_K_M** : Ã‰quilibre parfait vitesse/qualitÃ©

### ğŸŒ Interface Web Moderne
- **Design responsive** : Fonctionne sur desktop et mobile
- **Coloration syntaxique** : Support automatique pour tous les langages
- **Streaming temps rÃ©el** : RÃ©ponses en temps rÃ©el
- **Export de conversations** : Sauvegarde au format JSON
- **Monitoring matÃ©riel** : Surveillance CPU, RAM, GPU en temps rÃ©el
- **AccÃ¨s rÃ©seau local** : Accessible depuis tous les appareils du rÃ©seau

### ğŸ”§ API ComplÃ¨te
- **REST API** : Compatible OpenAI
- **WebSocket** : Communication bidirectionnelle
- **Streaming** : RÃ©ponses progressives
- **Documentation automatique** : Swagger UI intÃ©grÃ©

## ğŸ“‹ PrÃ©requis

### MatÃ©riel
- **CPU** : Intel i5 (ou Ã©quivalent)
- **GPU** : NVIDIA GTX 950M (4GB VRAM)
- **RAM** : 8GB minimum
- **Stockage** : 10GB d'espace libre

### Logiciel
- **OS** : Ubuntu 20.04+ (recommandÃ©)
- **Python** : 3.8+
- **CUDA** : 11.8+ (pour l'accÃ©lÃ©ration GPU)

## ğŸš€ Installation Rapide

### 1. Clonage et Installation
```bash
# Cloner le projet
git clone https://github.com/scareface972/llama-api-local.git
cd llma-api

# Installation automatique (Ubuntu)
chmod +x install.sh
./install.sh
```

### 2. TÃ©lÃ©chargement du ModÃ¨le
```bash
# TÃ©lÃ©chargement automatique du modÃ¨le optimisÃ©
chmod +x download_model.sh
./download_model.sh
```

### 3. DÃ©marrage du Serveur
```bash
# DÃ©marrage avec vÃ©rifications automatiques
chmod +x start_server.sh
./start_server.sh
```

### 4. AccÃ¨s Ã  l'Interface
- **Interface Web** : http://localhost:8000
- **Documentation API** : http://localhost:8000/docs
- **Health Check** : http://localhost:8000/health

## ğŸŒ AccÃ¨s RÃ©seau Local

### Configuration Automatique
L'API est configurÃ©e pour Ãªtre accessible sur le rÃ©seau local avec `host: "0.0.0.0"`.

### VÃ©rification de l'AccÃ¨s RÃ©seau
```bash
# Afficher les informations rÃ©seau
chmod +x network-info.sh
./network-info.sh
```

### AccÃ¨s depuis d'Autres Appareils
1. **Connectez-vous au mÃªme rÃ©seau WiFi/LAN**
2. **Ouvrez un navigateur**
3. **Tapez l'URL** : `http://[ADRESSE_IP_SERVEUR]:8000`

### Exemples d'URLs
- **Interface Web** : `http://192.168.1.100:8000`
- **API REST** : `http://192.168.1.100:8000/v1/chat/completions`
- **Documentation** : `http://192.168.1.100:8000/docs`
- **Health Check** : `http://192.168.1.100:8000/health`

### Configuration du Firewall
```bash
# Autoriser le port 8000
sudo ufw allow 8000/tcp

# VÃ©rifier le statut
sudo ufw status
```

### SÃ©curitÃ© RÃ©seau
âš ï¸ **IMPORTANT** : L'API est accessible sur le rÃ©seau local
- Assurez-vous que votre rÃ©seau est sÃ©curisÃ©
- Changez le port si nÃ©cessaire dans config.py
- Configurez un firewall appropriÃ©
- Utilisez HTTPS en production

## ğŸ“ Structure du Projet

```
llma-api/
â”œâ”€â”€ ğŸ“„ install.sh              # Script d'installation automatique
â”œâ”€â”€ ğŸ“„ uninstall.sh            # Script de dÃ©sinstallation
â”œâ”€â”€ ğŸ“„ download_model.sh       # TÃ©lÃ©chargement du modÃ¨le
â”œâ”€â”€ ğŸ“„ start_server.sh         # DÃ©marrage du serveur
â”œâ”€â”€ ğŸ“„ daemon-control.sh       # ContrÃ´le du service systemd
â”œâ”€â”€ ğŸ“„ network-info.sh         # Informations rÃ©seau âœ¨ NOUVEAU
â”œâ”€â”€ ğŸ“„ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ config.py              # Configuration optimisÃ©e
â”œâ”€â”€ ğŸ“„ llama_api.py           # API FastAPI principale
â”œâ”€â”€ ğŸ“„ README.md              # Documentation
â”œâ”€â”€ ğŸ“ models/                # ModÃ¨les IA
â”‚   â””â”€â”€ llama-2-7b-chat.gguf  # ModÃ¨le principal
â”œâ”€â”€ ğŸ“ templates/             # Templates HTML
â”‚   â””â”€â”€ index.html            # Interface web
â”œâ”€â”€ ğŸ“ static/                # Fichiers statiques
â”œâ”€â”€ ğŸ“ logs/                  # Logs de l'application
â””â”€â”€ ğŸ“ llama.cpp/             # BibliothÃ¨que llama.cpp
```

## âš™ï¸ Configuration OptimisÃ©e

### ParamÃ¨tres MatÃ©riels
```python
HARDWARE_CONFIG = {
    "cpu_threads": 4,        # CÅ“urs i5 optimisÃ©s
    "gpu_layers": 20,        # Couches GPU pour GTX 950M
    "ram_gb": 8,            # RAM systÃ¨me
    "vram_gb": 4,           # VRAM GPU
    "batch_size": 512,      # Batch optimisÃ©
    "context_size": 2048,   # Contexte adaptÃ©
}
```

### Optimisations Automatiques
- **DÃ©tection CPU** : Nombre de cÅ“urs automatique
- **Gestion RAM** : Ajustement selon la mÃ©moire disponible
- **Optimisation GPU** : Utilisation maximale de la VRAM
- **Quantisation** : Q4_K_M pour Ã©quilibre performance/mÃ©moire

## ğŸŒ Utilisation de l'API

### Interface Web
1. Ouvrez http://localhost:8000 (ou l'IP du serveur)
2. Tapez votre message dans la zone de texte
3. Utilisez Ctrl+Enter pour envoyer rapidement
4. Ajustez les paramÃ¨tres dans la sidebar

### API REST
```bash
# Conversation simple
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Bonjour !"}],
    "temperature": 0.8,
    "max_tokens": 2048
  }'

# Streaming
curl -X POST "http://localhost:8000/v1/chat/completions/stream" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Explique-moi Python"}],
    "stream": true
  }'
```

### WebSocket
```javascript
const ws = new WebSocket('ws://localhost:8000/ws/chat');
ws.send(JSON.stringify({
    messages: [{role: "user", content: "Hello!"}],
    temperature: 0.8
}));
```

## ğŸ¨ FonctionnalitÃ©s de l'Interface

### Coloration Syntaxique
- **Support automatique** : Python, JavaScript, C++, etc.
- **ThÃ¨me sombre** : Prism.js avec thÃ¨me Tomorrow
- **Copie facile** : Bouton de copie intÃ©grÃ©

### ParamÃ¨tres AvancÃ©s
- **TempÃ©rature** : 0.0 - 2.0 (contrÃ´le de la crÃ©ativitÃ©)
- **Tokens max** : 256 - 4096 (longueur des rÃ©ponses)
- **Prompt systÃ¨me** : Instructions personnalisÃ©es

### Monitoring Temps RÃ©el
- **CPU** : Utilisation en pourcentage
- **RAM** : MÃ©moire systÃ¨me utilisÃ©e
- **ModÃ¨le** : Ã‰tat de chargement
- **Temps de rÃ©ponse** : Latence en millisecondes

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes Courants

#### ModÃ¨le non trouvÃ©
```bash
# VÃ©rifiez que le modÃ¨le est tÃ©lÃ©chargÃ©
ls -la models/
# Si absent, relancez le tÃ©lÃ©chargement
./download_model.sh
```

#### Erreur CUDA
```bash
# VÃ©rifiez l'installation CUDA
nvidia-smi
# RÃ©installez si nÃ©cessaire
sudo apt install nvidia-cuda-toolkit
```

#### MÃ©moire insuffisante
```bash
# VÃ©rifiez la RAM disponible
free -h
# Fermez d'autres applications
# Ou rÃ©duisez batch_size dans config.py
```

#### Port dÃ©jÃ  utilisÃ©
```bash
# Changez le port dans config.py
# Ou tuez le processus existant
sudo lsof -ti:8000 | xargs kill -9
```

#### ProblÃ¨mes d'accÃ¨s rÃ©seau
```bash
# VÃ©rifiez les informations rÃ©seau
./network-info.sh

# Ouvrez le port firewall
sudo ufw allow 8000/tcp

# VÃ©rifiez le statut du service
sudo systemctl status llama-api
```

### Logs et Debug
```bash
# Consultez les logs
tail -f logs/api.log

# Mode debug
export LOG_LEVEL=DEBUG
./start_server.sh

# Logs du service systemd
sudo journalctl -u llama-api -f
```

## ğŸ“Š Performances

### Benchmarks (Configuration i5 + GTX 950M)
- **Chargement modÃ¨le** : ~30 secondes
- **PremiÃ¨re rÃ©ponse** : ~2-5 secondes
- **RÃ©ponses suivantes** : ~1-3 secondes
- **Utilisation RAM** : ~6-7GB
- **Utilisation VRAM** : ~3.5GB

### Optimisations AppliquÃ©es
- **MMAP** : Chargement mÃ©moire optimisÃ©
- **F16 KV Cache** : RÃ©duction utilisation VRAM
- **Batch Processing** : Traitement par lots
- **Thread Pooling** : Utilisation CPU optimisÃ©e

## ğŸ”’ SÃ©curitÃ©

### Configuration SÃ©curisÃ©e
- **CORS** : ConfigurÃ© pour dÃ©veloppement local
- **Rate Limiting** : 100 requÃªtes/minute
- **Input Validation** : Validation Pydantic
- **Error Handling** : Gestion d'erreurs robuste

### Production
```bash
# Pour la production, modifiez config.py
SECURITY_CONFIG = {
    "cors_origins": ["https://votre-domaine.com"],
    "rate_limit": 50,
    "max_tokens_per_request": 2048,
}
```

## ğŸ¤ Contribution

### DÃ©veloppement Local
```bash
# Installation dÃ©veloppement
pip install -r requirements.txt
pip install -e .

# Tests
python -m pytest tests/

# Linting
flake8 llama_api.py config.py
```

### Ajout de ModÃ¨les
1. TÃ©lÃ©chargez le modÃ¨le GGUF
2. Placez-le dans `models/`
3. Mettez Ã  jour `config.py`
4. RedÃ©marrez le serveur

## ğŸ“ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ™ Remerciements

- **llama.cpp** : Georgi Gerganov et la communautÃ©
- **FastAPI** : SebastiÃ¡n RamÃ­rez
- **Hugging Face** : ModÃ¨les et infrastructure
- **TheBloke** : ModÃ¨les quantifiÃ©s

## ğŸ“ Support

Pour toute question ou problÃ¨me :
1. Consultez la section dÃ©pannage
2. VÃ©rifiez les logs dans `logs/api.log`
3. Utilisez `./network-info.sh` pour les problÃ¨mes rÃ©seau
4. Ouvrez une issue sur GitHub

---

**ğŸ‰ Votre IA locale est prÃªte ! Profitez de conversations intelligentes sans dÃ©pendance externe.** 
