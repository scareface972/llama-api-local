# ü§ñ API Llama.cpp - Installation Propre

Une API REST compl√®te pour l'inference de mod√®les de langage avec llama.cpp, optimis√©e pour Ubuntu et facile √† installer.

## üéØ Caract√©ristiques

- **API REST** avec FastAPI
- **Interface web moderne** avec chat en temps r√©el
- **Support multi-mod√®les** (Llama-2, CodeLlama, Mistral, etc.)
- **Optimisations CPU** pour performances maximales
- **Service systemd** pour d√©marrage automatique
- **Interface r√©seau** accessible depuis d'autres appareils
- **Monitoring syst√®me** int√©gr√©

## üöÄ Installation Rapide

### 1. D√©sinstallation (si n√©cessaire)
```bash
# Si vous avez une installation pr√©c√©dente
./uninstall.sh
```

### 2. Installation propre
```bash
# Installation compl√®te
./install_clean.sh
```

### 3. T√©l√©chargement du mod√®le
```bash
# T√©l√©chargement interactif d'un mod√®le
./download_model.sh
```

### 4. D√©marrage du serveur
```bash
# D√©marrage manuel
./start_server.sh

# Ou d√©marrage en service
sudo systemctl start llama-api
```

## üìã Pr√©requis

- **OS** : Ubuntu 20.04+ (test√© sur Ubuntu 24.04)
- **RAM** : 8GB minimum (16GB recommand√©)
- **Espace disque** : 10GB minimum
- **CPU** : x86_64 avec support AVX2
- **R√©seau** : Connexion internet pour t√©l√©charger les mod√®les

## üîß Scripts Disponibles

| Script | Description |
|--------|-------------|
| `install_clean.sh` | Installation compl√®te et propre |
| `uninstall.sh` | D√©sinstallation compl√®te |
| `download_model.sh` | T√©l√©chargement interactif de mod√®les |
| `start_server.sh` | D√©marrage du serveur |
| `network-info.sh` | Informations r√©seau |

## üåê Acc√®s

Une fois le serveur d√©marr√© :

- **Interface Web** : http://localhost:8000
- **Documentation API** : http://localhost:8000/docs
- **Health Check** : http://localhost:8000/health
- **Acc√®s r√©seau** : http://[VOTRE_IP]:8000

## üì¶ Mod√®les Support√©s

Le script `download_model.sh` propose plusieurs mod√®les :

1. **Llama-2-7B-Chat** (4.37 GB) - Recommand√© pour d√©buter
2. **Llama-2-13B-Chat** (7.87 GB) - Plus performant, plus lent
3. **Llama-2-7B** (4.37 GB) - Mod√®le de base
4. **CodeLlama-7B-Instruct** (4.37 GB) - Sp√©cialis√© code
5. **Mistral-7B-Instruct** (4.37 GB) - Tr√®s performant
6. **T√©l√©chargement personnalis√©** - URL personnalis√©e

## üõ†Ô∏è Gestion du Service

```bash
# D√©marrer le service
sudo systemctl start llama-api

# Arr√™ter le service
sudo systemctl stop llama-api

# Red√©marrer le service
sudo systemctl restart llama-api

# V√©rifier le statut
sudo systemctl status llama-api

# Voir les logs
sudo journalctl -u llama-api -f

# Activer le d√©marrage automatique
sudo systemctl enable llama-api
```

## üîç D√©pannage

### Probl√®mes courants

1. **Port 8000 d√©j√† utilis√©**
   ```bash
   sudo lsof -i :8000
   sudo kill -9 [PID]
   ```

2. **Mod√®le non trouv√©**
   ```bash
   ./download_model.sh
   ```

3. **D√©pendances manquantes**
   ```bash
   ./install_clean.sh
   ```

4. **Permissions insuffisantes**
   ```bash
   chmod +x *.sh
   ```

### Logs et diagnostic

```bash
# Logs de l'application
tail -f logs/llama_api.log

# Logs syst√®me
sudo journalctl -u llama-api -f

# Informations syst√®me
./network-info.sh
```

## üîí S√©curit√©

- L'API est accessible sur le r√©seau local (0.0.0.0:8000)
- Configurez votre firewall si n√©cessaire
- Pour un acc√®s externe, utilisez un reverse proxy avec authentification

## üìä Performance

### Configuration recommand√©e
- **CPU** : 4+ c≈ìurs
- **RAM** : 16GB
- **Espace disque** : SSD recommand√©
- **R√©seau** : 100Mbps minimum

### Optimisations automatiques
- Compilation optimis√©e pour votre CPU
- Gestion intelligente de la m√©moire
- Quantisation Q4_K_M pour √©quilibre performance/taille

## ü§ù Contribution

Ce projet est maintenu par la communaut√©. Pour contribuer :

1. Fork le projet
2. Cr√©ez une branche pour votre fonctionnalit√©
3. Committez vos changements
4. Poussez vers la branche
5. Ouvrez une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de d√©tails.

## üôè Remerciements

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Moteur d'inference
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web
- [TheBloke](https://huggingface.co/TheBloke) - Mod√®les quantifi√©s
- [Hugging Face](https://huggingface.co/) - H√©bergement des mod√®les

---

**üéØ Votre API Llama.cpp est pr√™te !** 

Pour commencer, ex√©cutez simplement :
```bash
./install_clean.sh
```

## üîß **Correction des Options CMake**

### **Probl√®me identifi√© :**
- Les variables CMake utilis√©es √©taient **obsol√®tes** ou **incorrectes**
- `LLAMA_AVX`, `LLAMA_AVX2`, `LLAMA_F16C`, `LLAMA_FMA` ne sont plus utilis√©es
- Il faut utiliser les **nouvelles options** de la version r√©cente de llama.cpp

### **Corrections apport√©es :**

1. **Options CMake corrig√©es** :
   ```bash
   # Anciennes (incorrectes)
   -DLLAMA_AVX=ON -DLLAMA_AVX2=ON -DLLAMA_F16C=ON -DLLAMA_FMA=ON
   
   # Nouvelles (correctes)
   -DLLAMA_BLAS=ON -DLLAMA_OPENBLAS=ON -DLLAMA_NATIVE=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_METAL=OFF -DLLAMA_CUBLAS=OFF
   ```

2. **Scripts mis √† jour** :
   - ‚úÖ `install_clean.sh` - Installation compl√®te
   - ‚úÖ `compile_llama.sh` - Compilation rapide
   - ‚úÖ `check_cmake_options.sh` - Diagnostic des options

### **Nouvelles options expliqu√©es :**
- `LLAMA_BLAS=ON` - Support BLAS pour acc√©l√©ration
- `LLAMA_OPENBLAS=ON` - Utilisation d'OpenBLAS
- `LLAMA_NATIVE=ON` - Optimisations natives pour votre CPU
- `LLAMA_BUILD_SERVER=ON` - Compilation du serveur
- `LLAMA_METAL=OFF` - D√©sactive Metal (macOS)
- `LLAMA_CUBLAS=OFF` - D√©sactive CUDA (CPU uniquement)

## üöÄ **Prochaines √âtapes**

Maintenant que les options CMake sont corrig√©es, vous pouvez :

### **Option 1 : V√©rifier les options disponibles**
```bash
./check_cmake_options.sh
```

### **Option 2 : Relancer l'installation**
```bash
./install_clean.sh
```

### **Option 3 : Compiler seulement llama.cpp**
```bash
./compile_llama.sh
```

## ‚úÖ **Avantages de la Correction**

- ‚úÖ **Options CMake valides** - Plus d'avertissements
- ‚úÖ **Compilation optimis√©e** - Utilisation des bonnes optimisations
- ‚úÖ **Compatibilit√©** - Fonctionne avec la version r√©cente de llama.cpp
- ‚úÖ **Diagnostic** - Script pour v√©rifier les options disponibles

Voulez-vous maintenant relancer l'installation avec les bonnes options CMake ?

# API Llama.cpp Local

API Python locale utilisant llama.cpp optimis√©e pour un serveur Ubuntu d√©di√© avec processeur i5, 8GB RAM et carte Nvidia GTX 950M 4GB VRAM.

## üöÄ Installation

### Installation compl√®te
```bash
./install_clean.sh
```

### T√©l√©chargement du mod√®le
```bash
./download_model.sh
```

### D√©marrage du serveur
```bash
./start_server.sh
```

## üî® Compilation de llama.cpp

Si vous rencontrez des probl√®mes de compilation, utilisez l'une de ces m√©thodes :

### 1. Compilation par d√©faut (recommand√©e)
```bash
./compile_default.sh
```

### 2. Compilation simple
```bash
./compile_simple.sh
```

### 3. Compilation rapide
```bash
./compile_llama.sh
```

### 4. V√©rification des options CMake
```bash
./check_cmake_options.sh
```

## üìã Scripts disponibles

- `install_clean.sh` - Installation compl√®te et propre
- `download_model.sh` - T√©l√©chargement interactif des mod√®les
- `start_server.sh` - D√©marrage du serveur avec v√©rifications
- `compile_default.sh` - Compilation avec configuration par d√©faut
- `compile_simple.sh` - Compilation simple
- `compile_llama.sh` - Compilation rapide
- `check_cmake_options.sh` - V√©rification des options CMake
- `uninstall.sh` - D√©sinstallation compl√®te
- `network-info.sh` - Informations r√©seau

## üåê Acc√®s

- **Interface Web** : http://localhost:8000
- **Documentation API** : http://localhost:8000/docs
- **Health Check** : http://localhost:8000/health

## üîß Gestion du service

```bash
# D√©marrer le service
sudo systemctl start llama-api

# Arr√™ter le service
sudo systemctl stop llama-api

# Red√©marrer le service
sudo systemctl restart llama-api

# V√©rifier le statut
sudo systemctl status llama-api

# Voir les logs
sudo journalctl -u llama-api -f
```

## üìù Notes importantes

- **Configuration CMake** : Utilise la configuration par d√©faut sans options sp√©cifiques pour √©viter les avertissements
- **Environnement virtuel** : Cr√©√© automatiquement dans le dossier `venv/`
- **Mod√®les** : T√©l√©charg√©s dans le dossier `models/`
- **Logs** : Stock√©s dans le dossier `logs/`

## üõ†Ô∏è D√©pannage

### Probl√®mes de compilation
1. Utilisez `./compile_default.sh` pour une compilation sans options
2. V√©rifiez que CMake est install√© : `sudo apt install cmake`
3. Nettoyez et recommencez : `rm -rf llama.cpp/build && ./compile_default.sh`

### Probl√®mes de d√©pendances
1. R√©installez l'environnement virtuel : `rm -rf venv && python3 -m venv venv`
2. Activez l'environnement : `source venv/bin/activate`
3. Installez les d√©pendances : `python -m pip install -r requirements.txt`

### Probl√®mes de service
1. V√©rifiez les logs : `sudo journalctl -u llama-api -f`
2. Red√©marrez le service : `sudo systemctl restart llama-api`
3. V√©rifiez les permissions : `sudo chown -R $USER:$USER .`
